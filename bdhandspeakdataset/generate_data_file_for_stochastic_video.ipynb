{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b827c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "irrelevant_const = \"IRRELEVANT\"\n",
    "\n",
    "sign_subtitle_json_file_root = \"../extractedbdhandspeakskeletons/skeleton_subtitle_jsons\"\n",
    "video_embeddings_root = \"../extractedbdhandspeakskeletons/video_embeddings_cropped\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(sign_subtitle_json_file_root) if isfile(join(sign_subtitle_json_file_root, f))]\n",
    "\n",
    "def generate_sequence_id(line):\n",
    "    p = re.compile('[a-zA-Z0-9\\-]')\n",
    "    return \"\".join(p.findall(line))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "keys_of_skeleton = []\n",
    "properties_of_consideration = [{\n",
    "    'name': 'pose_keypoints_2d', 'count': 25\n",
    "}, {\n",
    "    'name': 'face_keypoints_2d', 'count': 70\n",
    "}, {\n",
    "    'name': 'hand_left_keypoints_2d', 'count': 21\n",
    "}, {\n",
    "    'name': 'hand_right_keypoints_2d', 'count': 21\n",
    "}]\n",
    "\n",
    "total_body_keypoint_count = 0\n",
    "for property in properties_of_consideration:\n",
    "    property_name = \"_\".join(property['name'].split('_')[:-2])\n",
    "    for i in range(property['count']):\n",
    "        keys_of_skeleton.append(f\"{ property_name }_{i}_x\")\n",
    "        keys_of_skeleton.append(f\"{ property_name }_{i}_y\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f08c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_array(list_of_videos):\n",
    "    final_array = []\n",
    "    for json_file_name in tqdm(list_of_videos):\n",
    "        json_file_object = open(f\"{sign_subtitle_json_file_root}/{json_file_name}\", encoding='utf-8')\n",
    "    \n",
    "        json_dict = json.load(json_file_object)\n",
    "\n",
    "        video_embedding_pickle_file_path = f\"{video_embeddings_root}/{json_file_name.split('.')[0]}.avi.pickle\"\n",
    "        with open(video_embedding_pickle_file_path, 'rb') as f:\n",
    "            video_embeddings = pickle.load(f)\n",
    "        \n",
    "        for single_sentence_data in json_dict['skeleton_data']:\n",
    "            sequence_id = f\"{json_dict['video_name']}%%{single_sentence_data['english']}%%{single_sentence_data['start_time']}-{single_sentence_data['end_time']}%%\"\n",
    "\n",
    "\n",
    "            sign_embeddings = torch.clone(video_embeddings[ single_sentence_data['start_time']*10 : (single_sentence_data['end_time']+1)*10 ])\n",
    "\n",
    "            # sign_array_of_single_sentence = []\n",
    "\n",
    "            # skeletons_of_single_sentence = single_sentence_data['skeletons']\n",
    "\n",
    "            # for single_skeleton in skeletons_of_single_sentence:\n",
    "            #     single_skeleton_numbers = []\n",
    "            #     for key_of_skeleton in keys_of_skeleton:\n",
    "            #         single_skeleton_numbers.append(single_skeleton[key_of_skeleton])\n",
    "            #     sign_array_of_single_sentence.append(single_skeleton_numbers)\n",
    "\n",
    "            final_array.append({\n",
    "                \"name\": sequence_id,\n",
    "                \"signer\": irrelevant_const,\n",
    "                \"gloss\": irrelevant_const,\n",
    "                # \"sign\": torch.Tensor(sign_array_of_single_sentence),\n",
    "                \"sign\": sign_embeddings,\n",
    "                \"text\": single_sentence_data['bengali']\n",
    "            })\n",
    "\n",
    "        json_file_object.close()\n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1031f87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [02:21<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "whole_dataset = generate_final_array(onlyfiles)\n",
    "\n",
    "random.shuffle(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca918c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d89aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, non_train_set = train_test_split(whole_dataset,test_size=0.3, random_state=42)\n",
    "test_set, validation_set = train_test_split(non_train_set, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060d5dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7027\n",
      "1506\n",
      "1506\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "372885f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_pickle_handler = open(\"data_for_stochastic/phoenix14t.pami0.train\",\"wb\")\n",
    "validation_array_pickle_handler = open(\"data_for_stochastic/phoenix14t.pami0.dev\",\"wb\")\n",
    "test_array_pickle_handler = open(\"data_for_stochastic/phoenix14t.pami0.test\",\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0df39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping train pickles\n",
      "dumping test pickles\n",
      "dumping dev pickles\n"
     ]
    }
   ],
   "source": [
    "print(\"dumping train pickles\")\n",
    "pickle.dump(train_set, train_array_pickle_handler)\n",
    "print(\"dumping test pickles\")\n",
    "pickle.dump(test_set, test_array_pickle_handler)\n",
    "print(\"dumping dev pickles\")\n",
    "pickle.dump(validation_set, validation_array_pickle_handler)\n",
    "\n",
    "train_array_pickle_handler.close()\n",
    "validation_array_pickle_handler.close()\n",
    "test_array_pickle_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a9477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2948e2135441f7c17828407a457c54d0f2e9d2c96cc787101a5a0573b6a04d73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
